{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "import io\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import GRU\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filename):\n",
    "  df = pd.read_excel(filename)\n",
    "  intent = df[\"intent\"]\n",
    "  unique_intent = list(set(intent))\n",
    "  text = list(df[\"text\"])\n",
    "  return (intent, unique_intent, text, df)\n",
    "\n",
    "\n",
    "def load_word_vectors(filepath,vocab):\n",
    "    with zipfile.ZipFile(filepath) as zfile:\n",
    "        for finfo in zfile.infolist():\n",
    "            ifile = zfile.open(finfo)\n",
    "            textStream = io.TextIOWrapper(ifile, encoding='utf-8')\n",
    "            #n, d = map(int, textStream.readline().split())\n",
    "            #print(\"Number of tokens = \"+str(n))\n",
    "            data = {}\n",
    "            for line in textStream:\n",
    "                tokens = line.rstrip().split(' ')\n",
    "                word=tokens[0]\n",
    "                if(word in vocab):\n",
    "                    data[word] = np.asarray(tokens[1:], dtype='float32')\n",
    "            return data\n",
    "\n",
    "        \n",
    "def build_embedding_matrix(tok,vectors,embedding_size):\n",
    "    vocab_size=len(tok.word_index) + 1\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_size))\n",
    "    for word, i in tok.word_index.items():\n",
    "        embedding_vector = vectors.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "def get_vocab(sentences):\n",
    "    text_words=list(map(lambda x: text_to_word_sequence(x,lower=False),text))\n",
    "    vocab=set()\n",
    "    for words in text_words:\n",
    "        vocab.update(words)\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def prepare_training_data(text,intent,max_length,embedding_size):\n",
    "    tok = Tokenizer(lower=False)\n",
    "    tok.fit_on_texts(text)\n",
    "    encoded_docs = tok.texts_to_sequences(text)\n",
    "    padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "    embedding_matrix = build_embedding_matrix(tok,glove_vectors,embedding_size)\n",
    "    vocab_size = len(tok.word_index) + 1\n",
    "    le = LabelEncoder()\n",
    "    labels_encoded=le.fit_transform(intent)\n",
    "    ohe = OneHotEncoder(sparse=False,categories='auto')\n",
    "    output_one_hot = ohe.fit_transform(labels_encoded.reshape(-1, 1))\n",
    "    train_X, val_X, train_Y, val_Y = train_test_split(padded_docs, output_one_hot, shuffle = True, test_size = 0.2)\n",
    "    return (train_X, val_X, train_Y, val_Y, vocab_size, embedding_matrix)\n",
    "\n",
    "\n",
    "def build_model(embedding_matrix,max_length, use_pre_trained_vectors=True):\n",
    "    model = Sequential()\n",
    "    if use_pre_trained_vectors:\n",
    "        e = Embedding(vocab_size, embedding_size, weights=[embedding_matrix], \n",
    "                      input_length=max_length, trainable=False)\n",
    "    else:\n",
    "        e = Embedding(vocab_size, embedding_size, input_length=max_length)\n",
    "    \n",
    "    model.add(e)\n",
    "    model.add(Bidirectional(GRU(128)))\n",
    "    model.add(Dense(64, activation = \"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation = \"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(21, activation = \"softmax\"))\n",
    "    model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent, unique_intent, text, df = load_dataset(\"../data/chatbot_questions.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=get_vocab(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vectors=load_word_vectors(\"../data/pre-trained-vectors/crawl-300d-2M.vec.zip\",vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length=20\n",
    "embedding_size = 300\n",
    "\n",
    "train_X, val_X, train_Y, val_Y, vocab_size, embedding_matrix = prepare_training_data(text,intent,max_length,embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(embedding_matrix,max_length,use_pre_trained_vectors=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"../data/model/train-embedding/model-epoch-{epoch:02d}-val_acc-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_X, train_Y, epochs = 200, batch_size = 32, validation_data = (val_X, val_Y),callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
